 Olá e boas vindas a essa aula 2 da imersão de agentes de IA da Alura com o Google Gemini. E novamente, estamos aqui eu, Fabrício Carraro, também com a Carol Carneiro e o Vinícius Caridá pra segurar a sua mão nessa jornada e ir juntos aprender mais sobre agentes de inteligência artificial. Preparado, Vinícius? Preparada, Carol? Agora, pessoal. Que a parte boa vai ser agora. Sim, talvez uma das aulas a gente ainda não vai fazer a estrutura inteira do agente, como a gente comentou, né, no final da aula 1, mas hoje vai ser uma das partes mais interessantes que você pode usar tanto agora pro conteúdo dessa imersão quanto fora dela, que é a parte de criar o Rague, né? Então, aqui na tela que vocês estão vendo, do lado esquerdo, a primeira coisa que o agente vai fazer, né, que é quando ele meio que já tem a resposta ali em algum FAC, algum WIC interno, mas ele quer buscar lá em vez de buscar no conhecimento dele, né? Então, eu pensei da gente, quando a gente vai fazer essa estrutura desse Raguezinho, eu quero perguntar pro Gemini e lá no site do Gemini perguntar, né, olha, eu não entendi muito bem, a gente meio que explicou o que era, né, o que era, mas me explique, Gemini, me explique como se eu tivesse 5 anos de idade esse prompt clássico, né, que todo mundo usa como IA, o que é o Rague em termos de IA? Vamos ver o que ele vai dar pra gente aqui. Imagina que você quer saber por que o céu é azul. Você sabe que o robô sabe muitas coisas, mas só que ele aprendeu na escolinha dele o que foi o que o professor ensinou. Às vezes o robô pode não saber a resposta ou dar uma resposta que não faz sentido. Vamos ver de um jeito diferente. Você pergunta pro robô, mas antes dele responder, ele pega um monte de livrinhos coloridos que estão numa estante só com informações sobre o céu, as nuvens e as cores. Ele lê rapidinho esses livrinhos e com o que ele aprendeu neles, ele te dá uma resposta super legal e certa. O céu é azul por causa da luz do sol que bate nas coisas que tem no ar. Eu gostei dessa explicação, ficou legal. É isso que o Rague faz e a gente pode continuar aqui. Em vez de você pedir pro Gemini dar informação baseado sobre as minhas férias, baseado no que ele sabe, e que talvez ele vai inventar alguma coisa ali, eu vou passar pra ele algum faque, algum texto, algum PDF, que é como muitas empresas estruturam informações e ele vai ler essa coisa e gerar a resposta dele não baseado no conhecimento, mas baseado no que ele leu dentro desse PDF, desse livro, desse texto. Exato. Posso fazer um adendo? Pode, claro. Toda vez que eu penso em Rague, não sei se vou denunciar minha idade, mas quando eu era criança, do recente, eu usava enciclopédias para fazer trabalhos escolares. Então, digamos que eu sou o meu próprio modelo generativo. Me perguntaram quando foi a independência do Brasil, alguma coisa assim. Vou lá na enciclopédia, eu sei que aquele conteúdo está ali. O que eu vou fazer? Eu vou recuperar a informação da enciclopédia, vou ler, vou reforçar esse conhecimento. E o que eu vou fazer? Vou aumentar, porque eu não quero copiar o que está na enciclopédia. Então, eu vou aumentar aquele conteúdo com as minhas próprias palavras, com a minha criatividade e vou fazer meu trabalhinho. Eu acho que o Rague é exatamente isso. Com as informações da sua empresa, com as informações que você está conhecendo, você vai fazer uma pergunta, vai recuperar, vai aumentar dentro da sua janela de contexto e de temperatura e vai trazer uma resposta. E o Rague é bem bonito por causa disso. E se você quer falar com pessoas da geração Z, seria o Wikipedia. Não é enciclopédia de papel, podia ser o Wikipedia. Exato, pode ser o Wikipedia. Perfeito. Então, vamos lá para o código agora, diretamente eu vou criar aqui um bloquinho de texto dando um hashtag Aula2 mostrando que a gente está começando uma nova estrutura aqui, mas é o mesmo código basicamente. E provavelmente, se você começou a fazer esse código ontem, o seu colab aqui está desconectado e ele perdeu todo o contexto, todas as informações que ele tinha sobre o que é, por exemplo, LLM underline triagem, essas variáveis e mais. Então, você vai ter que, provavelmente, reconectar aqui em cima e você pode ir aqui, por exemplo, em ambiente de execução e clicar em executar tudo, que ele vai passar a executar tudo e vai colocar na memória tudo isso, o que é essa variável, LLM triagem, o que é a função triagem, todas essas coisas que a gente vai utilizar nas próximas aulas. Mas, bora lá para o código. O que a gente vai começar? Como que a gente vai fazer esse Rague? Installar as bibliotecas. Boa, como sempre, né? Eu acho que vale pensar assim, como foi colocado, o próprio Jim Inés explicou de uma maneira super simples, né? A teoria do Rague, ela não é muito complexa, é um pouco do que a gente comentou, mas por trás é o que nós vamos ver no código, tem alguns passos importantes. Por trás desses passos, eles são complexos, mas nós vamos usar bibliotecas que abstraem essa complexidade. Toda matemática, os cálculos que tem por trás vão ser abstraídos e essas bibliotecas vão ajudar a gente fazer essas reivindicações. Então, como vocês falaram, eu tenho que ir lá e buscar um livrinho igual o Jim Inés falou, eu tenho que buscar um documento. Então, eu tenho que ter uma biblioteca que me ajuda a ler o documento. Depois que eu li esse documento, a gente não precisa ler o documento por completo, eu posso quebrar esse documento em pedaços para ler só aqueles pedaços e assim por diante. Então, nós vamos importar essas bibliotecas que vão ajudar nessas etapas que tem por trás do Rague. A gente vai explicar em mais detalhes quando a gente chegar lá, né? Mas vamos importar, vamos instalar elas para começar a usar. Nesses pedacinhos, o legal aqui é fazer um paper stall com uma exclamação na frente, para a gente não ficar vendo um monte de coisas que estão processando, menos o que você comentou da aula passada, upgrade, essas coisas. E agora a gente vai importar o Lang chain community, porque o Lang chain tem várias integrações que a gente consegue fazer com recursos, né? Como por exemplo, o Vini comentou sobre os pedacinhos. A gente já falou sobre vetorização, busca semântica? Então, aqui a gente vai conseguir fazer o Lang chain community e o faz traço CPU para busca semântica. Mais algum, Vini? Tem mais algumas importantes, né? Tem o Lang chain text splitter, que é a questão de quebrar o texto em vários pedaços, que a gente chama de chunk, em inglês. E tem também o PYMU PDF, porque também a gente quer ler PDF. Então, aqui a gente está importando biblioteca PYMU PDF para ler PDF, o Lang chain text splitter para quebrar esse PDF em pequenos pedaços, o faz GPU. Uma vez que eu quebrei em pequenos pedaços, como que eu faço essa... A vetorização em música semântica? A similaridade, eu vejo a similaridade entre esses pedaços textos, nós vamos explicar daqui a pouco. E o Lang chain community que ajuda a fazer essas conexões. Então, já estamos instalando, por enquanto, todas essas bibliotecas. Beleza, deu um erro aqui. Acho que escrevi errado. O Lang chain não é underline, deve ser tracinho. É, tracinho. Traco text, traco splitter. Isso. Agora foi. Beleza, enquanto isso eu já posso clicar... Ixi, não achou também. Ah, era um S, né? Traco text, traço splitters. Isso. Perfeito. E o próximo passo, né? Depois da instalação, vai ser quais são os documentos que a gente vai utilizar no nosso RIG? Eu criei pra isso aqui, Carol, uma pastinha com alguns documentos da nossa empresa aqui, que eu tinha chamado antes de Carraro Desenvolvimento. Desculpa, a gente pode chamar-se lá de Carraro, Caridad, Carneiro, alguma coisa assim. Olha só, os três com C. CCC. CCC. Gostei, gostei. E aí, a gente tem aqui um documento que a gente pode usar, por exemplo, da empresa. Eu fiz bem curtinho aqui, poderia ser muito maior do que isso. Na verdade, não seria um problema. Mas é um primeiro documento, PDF, de política de reembolsos gerais e de viagens também. Então, o reembolso requer uma nota fiscal e tem que ser submetido 10 dias corridos. A alimentação tem um limite de R$ 70 por dia por pessoa e não cobre bebidas alcoólicas. Transporte de táxi quando não tiver alguma outra alternativa. Internet pra home office tem subsídio de R$ 100 por mês. Cursos e certificações da Alura, por exemplo. E custos excepcionais, tipo de fancrias de bagagem tem outra coisa. O meu outro PDF aqui é sobre políticas de uso de e-mail e segurança da informação. Proibido encaminhar endereços pessoais, documentos classificados, anexos, fiches, retenções. Várias coisas específicas aqui de segurança. E políticas de home office é o meu terceiro documento PDF. A empresa tem um modelo híbrido de dois dias, os equipamentos são fornecidos pela empresa, obrigatório uso de VPN por causa de segurança, ergonomia, exceção aqui tem que ser formalizada via chamada RH. Enfim, várias coisas, mas são três arquivos diferentes, cada um específico sobre o seu tema. E novamente, esses são pequenos de uma página, o seu aí de casa, da sua empresa poderia ter 57 páginas que a lógica aqui funcionaria do mesmo jeito. Não mudaria uma linha de código aqui basicamente se você não quisesse. Beleza, então vamos voltar agora pro meu código. Aqui já instalamos as bibliotecas, como é que a gente começa agora? O primeiro passo é trazer esses documentos pro Colab. Boa. Então que a gente vai acessar, né? Então a gente vai fazer um upload aqui na lateralzinha. Tem essa pastinha aqui, né? Isso, exatamente. A gente pode passar direto. Então, eu tenho aqui em upload, eu tenho aqui essa pasta de imersão em agentes de ar, e esses três arquivos PDFs vão estar aqui embaixo dessa aula pra você baixar aí na sua casa e também fazer o upload, porque você vai seguir exatamente como a gente tá seguindo aqui, né? Então, ele dá até um aviso, ó, os arquivos desse ambiente de execução vão ser excluídos quando ele for encerrado. Ou seja, toda vez que a máquina do Google cair aqui do lado direito, como também vão cair o que amavariava, né? Ele vai perder esse contexto, ele vai perder e você vai ter que fazer esse upload de novo. Tem um jeito mais fácil de fazer isso, você pode até pedir pra ele, ó, gerar o código com em A, se você não quiser fazer cada vez essa importação, de você colocar esses três arquivos lá no seu Google Drive, e aí falar, ó, Google, importa pra mim sempre todos os arquivos PDF na pasta imersão alura, por exemplo, e aí ele vai dar um mount aqui, né, pra montar o seu Google Drive toda vez, pra você não ter que manualmente fazer isso do lado esquerdo, mas a gente vai fazer por enquanto manualmente aqui do lado esquerdo, porque a gente tá fazendo essa gravação aqui diretamente, mas caso você queira, tenha essa opção que você pode usar com a ajuda da IA, ou com a ajuda do pessoal lá do Discord também. Mas, bom, tá importado aqui, né, Carol? E agora a gente precisa saber o caminho desse arquivo. Isso aí, exato. Então, a primeira coisa, antes de colocar o caminho, nós instalamos as bibliotecas, agora a gente tem que importar elas pra gente poder usar agora e ler esses arquivos. Então, o primeiro que nós vamos importar é o patchlib, então vamos lá, from, patchlib import-patch, import-patch, isso que vai aqui ajudar a gente a facilitar o trabalho aqui com o caminho dos diretórios que a gente vai usar, e o langchain-community, como a Carol falou, que vai ajudar a gente a conectar as pecinhas, então, from langchain-community.document-loaders, que é para carregadores de documentos, import e aí o py-mu-pdf-loader, que a gente também instalou anteriormente. Se você digita py, ele já dá umas sugestões de todas as coisas que vão te auxiliar a tratar com PDFs que já existem lá conectados à comunidade do langchain. Então, essa não precisa instalar diretamente, né, mas tá aqui, beleza, essa é a conexão basicamente, do py-mu-pdf com o langchain. Eu vou fechar isso aqui do lado esquerdo pra voltar pra galera inteira, mas eles estão aqui guardadinhos, tá? Feito as importações aqui, né, das libs que nós vamos usar, a primeira coisa é, eu vou ler documentos, eu preciso pegar esses documentos que a gente acabou de fazer o upload e salvar ele em algum lugar, então nós vamos colocar ele, como pode ser vários documentos, né, a gente coloca ele numa lista e salva esses documentos na lista. Então vamos criar essa lista vazia pra depois incluir esses documentos na lista. Depois chamar a lista de docs, que são os documentos que nós vamos salvar, é uma lista vazia. Então abre e fecha colchetes. E agora sim, como eu não sei quantos documentos, a gente tá importando três, você na sua casa pode querer testar com cinco, com dez, com cinquenta, enfim, então nós vamos criar uma interação pra ir lendo esses documentos e colocando na lista. Então vamos lá, vamos fazer um for de n documentos, n docs, que, só n é mais fácil, né, vamos simplificar o nome daqui da variável e aí nós vamos falar, vamos ler documentos que tá em algum lugar, nesse caso é o caminho da onde a gente salvou os documentos, então põe in espaço path, que é o caminho, e agora entre parênteses desse path, a gente vai colocar entre aspas, desculpa, barra content barra. Content ou contents? Content, só content. Essa pastinha é onde a gente, do próprio user data, né, que é onde a gente fez o upload dos documentos, mas se fosse o drive ou outra forma, poderia mudar outra pasta, mudaria esse caminho. Então a gente vai ler tudo que tá nesse caminho, como a Carol falou, que é o content, é esse espaço aqui do Google Collab, se tiver no drive ou em algum outro sistema. Uma pastinha que ele pode criar, a pessoa pode criar ali rapidinho. Aqui ó, tem uma pasta que ele já vem no Collab, nessa sample data, que ela tá dentro dessa content, então se você colocasse aqui dentro seria barra content barra sample underline data, porque você acionaria as coisas que estão aqui dentro, no caso a gente tá na raiz, então é só o content. Beleza, dois pontos. E aí o que a gente vai fazer? A gente tá lendo nesse caminho, a gente quer ler tudo que tem lá, mas pode ter um monte de arquivo, então a gente quer ler só PDF, tudo que tem ali de PDF, então a gente complementa só colocando ponto, globe, é um método, imagina né? É isso aí, você abre e fecha parênteses, e dentro dele entre aspas a gente põe asterisco ponto PDF. Beleza, tudo que for PDF lá dentro de content. E dependente do nome, o que porta ali é a terminação né? Sim, o sufixo é a terminaçãozinho. Legal, então criamos essa interação, o que nós vamos fazer a cada documento que ele encontra? Então pode dar um try, dois pontos, então cada documento que ele encontra, a primeira coisa que eu vou fazer é carregar a nossa biblioteca de ler documentos. Então você coloca aí loader, que é de carregar, igual o pymu.pdf.loader, que é o carinha que a gente importou. E aí entre parênteses a gente vai colocar str, que é o string, e entre parênteses de novo o número n. Não, não é número não, a letra N que é os documentos que nós estamos iterando. Então é cada uma né? Tem 15 arquivos lá dentro do content, que são do tipo PDF. Ele vai guardar cada um deles, a cada interação do for ele vai gravar esse artigo aqui em N. Então eu estou pegando aqui, converte para string, isso aí que você pegou, e abre usando o loader, e grava nessa variável, agora ele está no loader. Beleza, então a gente carregou. Aí o próximo passo é... Estrair. É, agora eu tenho que extrair essa informação. Dentro do try mesmo né? Isso, dentro do try. A gente vai fazer vários passos ali dentro. Então agora nós vamos pegar docs, que é a nossa lista que nós criamos. Aqui em cima né? Isso, é a nossa lista vazia, e vamos pôr ponto extended. Extended aqui. Isso. Vamos colocar uma coisa nova na lista basicamente, vamos no final da lista? Vamos sempre colocar uma coisa nova. E o que nós vamos colocar ali? O que está salvo no loader que a gente acabou de ler o arquivo. Então a gente vai pôr loader, ponto load, e abre e fecha parentes. Load aqui. Isso. E são parentes, porque é legal usar o try? Porque assim, a gente vai estar ali fazendo esse loop pra gente carregar todos os pdfs. Mas e se acontecer um erro? Será que eu vou travar o processo? Não. A gente continua dando uma mensagem de erro, mas eu não quero prejudicar todos os arquivos que a gente está carregando. Porque a gente não sabe se vai dar erro e onde vai dar erro. Aí não tem nada a ver com IA, com Rack, isso é programação, Paito. Exato. É, básica. É legal esse ponto que a Carol trouxe, porque dado que a gente carregou esse documento e colocou na lista, que é o que a gente quer fazer, pode funcionar ou não. Então pra saber se funcionou, a gente pode printar uma mensagem logo na sequência. Nossa, bom e velho print. É, põe um print falando, oh, funcionou. Então a gente dá um print, podemos colocar um f-string que fica bonitinho, e põe carregado, carregado, arquivo, com sucesso, por exemplo, carregado arquivo. Aí o nome do arquivo, talvez? E depois põe o nome do arquivo, perfeito. A gente vai botar o loader. É o N, N.name. N é o arquivo que carregou e o name, perfeito. E se der errado? Se der errado? A gente cria uma sessão, accept. Isso. Accept, aí espaço, exception, S. Ah, lá em cima, né? Eu tô... Isso, na mesma linha do accept. Vai criar a sessão S, por exemplo, um Szinho, dois pontos. A gente pode printar uma mensagem pra informar que deu erro. Já sugere, a gente pode usar isso aqui, né? Erro, carregar o arquivo, Nname, e o erro é o erro E, no caso. Perfeito. Podemos? Aí é legal fazer um print final de todos os documentos que foram... A gente tá printando documento a documento, que funcionou e que não funcionou. E a gente pode fazer um print de tudo que foi carregado, por exemplo. Quantos foram, né? Quantos foram, alguma coisa assim. Beleza. Então print, mais uma vez F string, e a gente pode pôr o total de documentos carregados. E aí como que a gente sabe o total? A gente tá colocando cada documento que é carregado numa lista. É só printar o tamanho da lista que a gente vai saber quantos documentos estão carregados na lista. Ele já sugere aqui, né? Ah, lembra, legal. Lembra, comprimento. Perfeito. E tá desse dox. Então o que a gente tá fazendo aqui, vamos passo a passo, no for. Ele bota, já falei, bota lá no Loader, dá um extend, que poderia ser um append, enfim, mas... Popula a nossa lista dox aqui com o nome desse documento, né? Com o Loader dele também. Então já tá carregado. Já tá carregado, já tá gravado tudo nessa lista dox. E print, ó, consegui carregar esse cara. E aí no final eu quero ver quantos que foram carregados, então eu dou o tamanho dessa lista dox. Então temos três aqui, né? 3.pdf deveria dar o resultado número 3. Esses três funcionarem, vai dar 3. Vamos rodar, vamos ver aqui se funciona. Olha que beleza. Carregado com sucesso o arquivo política de reembolsos. O outro também política de uso de mail. O político é de home office, todos.pdf. Total de documentos carregados, três. Sucesso. Sucesso. Maravilhoso. Nenhum erro. Nenhum erro. Podemos continuar aqui com o nosso ragzinho. Legal. Então, como a gente falou do conceito do rag, primeira coisa é eu ter documento e conseguir ler o documento. Legal. A segunda coisa importante é que, apesar dos modelos cada vez mais terem janelas de contextos cada vez maiores, existem técnicas como em context learning, que você pode deixar uma janela muito grande de contexto, mas isso deixa o modelo mais lento, mais caro, porque você paga por token que você paga no modelo, então o rag faz todo o sentido. E aí então um processo do rag é quebrar esse documento em pequenos pedaços. Para eu só usar o pedaço que realmente faz sentido dado aquela pergunta do usuário. É isso que a gente vai fazer agora. Perfeito. E aí, Carol, qual que biblioteca que a gente vai usar para fazer essa quebra? Agora a importar o Lang chain, text splitters. Então, na verdade, a gente vai colocar o from o Lang chain, underline, text, underline. Aquele carinha que a gente instalou lá em cima, text splitters, esse cara aqui. A gente vai importar o recursive character text splitter. Tomara que recomende. Tá, tá, ele recomendou. Ele bota REC que ele já recomenda. Bacana. E aí a gente vai, agora, todo o documento que a gente importou, a gente vai dividir, a gente vai transformar os textos em pedacinhos menores, são os chunks. A gente vai definir uma janela de chunks. Então, por exemplo, eu quero um chunk de tamanho 300, 500. A gente vai definir esse tamanho. Mas entre os caracteres, no caso, né? Isso. E um ponto importante também que a gente vai montar essa linha já é o seguinte. Então a gente está dividindo o documento. Ele vai ter um tamanho X. Mas pra gente não perder o contexto de um chunk para o próximo, é uma boa prática. Aí a gente colocar pelo menos um pouquinho de overlap. Então o final de um chunk vai ser o início do próximo. Então são dois pontos que a gente vai decidir agora. Bacana. Mostrando isso na prática pra vocês, eu vou abrir aqui, né? Então imagina que a gente pega um tamanho de chunk ali de 200, 300, seja lá que for, né, os caracteres. Aí ele vai pegar, por exemplo, ó. Reembolso. Vou botar aqui no selecionador. Reembolso. Requer noto fiscal, deve ser submetido até dias corridos. Aí alimentação e viagem. Limite B. E aí cortou aqui o chunk. A gente não sabe qual é o limite, né? Então é bom a gente botar um overlapzinho aqui, né? Porque senão o próximo chunk começaria em RS 70 por dia por pessoa, mas ele não saberia que ele está relacionado à alimentação. Então o que a gente faz é pegar ali um pouco do anterior. E também um pouco do próximo pra fazer, pra ficar mais claro isso, né? Essa conexão de tudo com as outras coisas. E como que a gente faz isso na prática? A gente vai, a gente pode definir uma variável, por exemplo, o splitter. E o splitter vai ser igual ao que a gente acabou de importar. Esse recursive character, blá blá blá, né? Exato. Vai entre parênteses. Vai ser chunk, underline size. Ah, ele já até sugeriu aqui pra mim. Em maiúsculo mesmo ou em minúsculo? Eu uso em minúsculo. Tá, então eu vou fazer em minúsculo também. Igual, o tamanho que a gente definiu, por exemplo, 300, vírgula e o chunk overlap. Que vai ser o quanto que ele vai dar de overlap entre um pedacinho, o chunk e o próximo. Que pode ser 30. É, 10% talvez. É um padrão. Normalmente esse é um valor padrão. Não é uma bala de prata, depende muito do tipo de documento, etc. Mas esse é um valor padrão. Tamanho 300 e 10% de overlapping. Se o overlap for muito grande... Ficar maior do que o próprio negócio. E muito pequeno você perde o contexto. Então essa é uma boa prática. Perfeito. E como continua a partir daqui? Agora a gente definiu como que a gente vai dividir e splitar esse documento. Mas eu quero saber agora como que a gente vai ter cada chunk. Então a gente pode criar uma variável chamada chunk. Chunks, por exemplo. A gente pode colocar igual. Splitter, que a gente definiu aqui em cima. Ponto. Split. Underline documents. Entre parênteses a lista de documentos que a gente fez aqui na outra célula. Populamos aqui os nossos três documentos. Então a gente passa para esse método Split Documents. E aí ele vai respeitar os parâmetros que a gente definiu aqui em splitter. Nessa variável splitter. Ele vai quebrar cada documento em pedaços de 300 caracteres. Mas sempre mantendo um overlap de 30 caracteres. Essa é a ideia. Então vamos rodar essa célula aqui. E obviamente ele não fez nada. Ele só gravou ali nas variáveis. A gente pode visualizar esse chunk. O que vocês acham? Vamos ver. O que ele fez exatamente? Clicar aqui para dar um print em tudo de uma vez. E olha que é uma coisa meio louca. Mas dá para notar que tem aqui o Split Documents. Então separe os documentos. Ele separou aqui um documento com um monte de informação dentro. Metadados. Está usando o Google Docs Renderer. Aí o criador que está vazio. Source, a fonte. Content, o nome do nosso arquivo. Política de Reembolso de Viagem.pdf. O path. Total de páginas, uma página. Formato PDF 1.4. E aí o título, imersão. E cadê o conteúdo? Page content. O conteúdo da página. Política de Reembolso, Viagens e Despesas. Que vai ser o nosso título basicamente. E aqui tem uns barra N. Quebra de espaço. De texto. Reembolso requer. Nosso fiscal deve ser submetido. Até a despesa. Alimentação e viagens. Limite de 130 dias. Por pessoa. Bebidas alcoólicas não reembolsáveis. Então ele pegou meio que até tudo aqui. Até o final. O próximo também pegou. O próximo também. O próximo document. Mas é meio bizarro visualizar assim. O que é interessante. A visualização não é muito legal. Pode melhorar. Mas eu só queria reforçar quando a gente fala do chunks. E você trouxe um exemplo muito bom, Fabrício. Mas aqui. Imagina se a gente colocasse um chunk, por exemplo, de 200. Ele não pegaria o finalzinho. Bebidas alcoólicas não reembolsáveis. E aí isso poderia ser um problema lá na frente. Porque o cliente, o usuário pode perguntar. E não vinha esse pedaço da informação. Então calibrar o chunk. E ter também essa questão do overlapping. Vai ajudar muito nesse ponto. Vou fazer um exemplo aqui. Vou botar um chunk de 100. Bem pequenininho. E 100 overlap. Vamos ver o que ele vai dar. Ele reclamou aqui. Colocou overlap igual a zero? Ele colocou aqui overlap. Padrão dele é 200. Então vou botar o 100 e overlap igual a zero. Vamos ver se ele vai deixar a gente fazer assim. Deixou. E os chunks. Tem muito mais chunk, né? Claro. Porque a gente está só 100 de 300 e 100 overlap. Que tal a gente, pra visualizar menor cada chunk, a gente pode criar rapidinho um for. E visualizar chunk por chunk e comparar os chunks. Vamos aqui. Então for chunk. Eu vou deixar assim, né? Esse bonitão aqui. Isso. Pra vocês verem. Mas vamos fazer aqui embaixo um for. Então for chunk, pra cada chunk. Em chunk. Dentro de chunk. Dois pontos. O que a gente vai printar ali? Printa o chunk. Uhum. E um bar N. Uns separadourzinhos. Um monte de linha bar N. Alguma coisa assim. Só uma variável chunk mesmo. Isso. Que já vai... Já vai melhorar a visualização. Já vai melhorar bastante. Um monte de linha aqui. Um bar N no final. Então isso aqui eu consigo fazer aqui em cima mesmo. Né? Foi um bar N no... É, tudo bem. Já ficou bom. Já ficou, né, com a outra coisa? Já ficou bom. Então, nosso primeiro chunk é o page content. Ele pegou direto só o page content. Facilitou nossa vida aqui. Mas olha. Política de reembolsos, viagens e despesa. Aí o outro... O um... Reembolso. Requer nota fiscal e deve ser submetido em até dez dias corridos após a... Aí tem o despesa aqui, né? Ele tá pegando alguma... Bebidas alcoólicas não... São reembolsáveis lá no outro cara, né? Então já perdeu totalmente o contexto. Perfeito. Então é por isso que a gente bota aqui geralmente, né? Esse 303... E 30, perdão? Mas que você pode fazer como você quiser. Na sua casa, no seu caso, na sua casa, na sua casa. Mas que você pode fazer como você quiser. Na sua casa, no seu caso de uso e tudo mais. É conta pra gente, né? Com a apoiar proporção que você encontrou. Sim. Eu vou até dar uma... Uma meia... Não é se dica, né? Mas é uma coisa que a gente pode fazer aqui. Que a gente tá usando todas essas ferramentas do framework, do Lang Team, que já são prontas. Eles já abstraem toda essa lógica pra você. Mas, em algum caso, talvez você tenha que ter um controle muito maior sobre isso. E aí você pode fazer essa parte aqui de splitar os chunks, né? De separar os chunks de forma manual. Sem utilizar esse Recursive Character Tag Splitter, né? Perfeito. Aí você pode fazer, por exemplo, em pandas, né? Você vai fazer uma tabelinha aqui básica em pandas. Você vai fazer, olha, pra cada parágrafo... O meu chunk não vai importar o tamanho. Não vai ser 300. Mas vai ser o parágrafo. E aí você vai ter um chunk com comprimento 327. O outro comprimento 265. Mas se isso for o melhor pro seu caso de uso, beleza. Inclusive, e aí dá pra você ir inteirando e fazendo coisas mais complexas. Por exemplo, você pode ler uma página e por contextos, por similaridade quebrar... Aí dá pra... Quando você faz de forma manual, você tem muito mais liberdade, mas aumenta a sua complexidade, né? Ou você pode usar uma biblioteca como essa aqui que facilita bastante. Perfeito. Mas vamos continuar aqui, construir o nosso rug utilizando a biblioteca do LangTran. Perfeito. E o próximo passo é uma parte que eu adoro, que é a parte de embeddings. Opa! Essa eu gosto também. Então, o que a gente precisa fazer? A gente já separou em vários pedacinhos. E a gente vai fazer agora... A gente quer realizar uma busca e de acordo com o que a gente buscou, a gente quer entender qual que é, dentro desse espaço que a gente vai criar de vetores baseado nos chunks que a gente tem. Vários caras, né? Esses vários chunks que a gente tem aqui. A gente quer saber qual que é a proximidade das coisas. Será que a Carol e Capivara tá perto uma da outra? A gente precisa transformar isso tudo em vetores. Eu acho que a gente pode até dar um passo atrás nisso, porque esse é um conceito muito importante em inteligência artificial, não só em agentes, mas em A em geral, né? NLNs, enfim... No mundo de NLP ou PLN, processamento de linguagem natural, que é uma das sub-áreas da IA. E pra isso eu vou abrir até um livro que eu publiquei junto com a Casa do Código, que é aqui do nosso grupo, o Alun, falando exatamente sobre inteligência artificial e tem uma sessão específica sobre o que são embeddings, explicando o meio passo a passo. Então eu vou abrir, já deixei aberto essa capa, inteligência artificial e chat GPT, a evolução dos modelos de IA generativa e a engenharia de prompt, a edição ampliada e atualizada, uma nova versão que a gente tem aqui. E eu vou lá na página 85, que é quando a gente tem aqui falando sobre word embeddings em redes neurais e tudo mais. Mas eu acho que, Carol, esse é o melhor exemplo pra gente explicar. Quando a gente tá treinando modelos de IA, a gente tem palavras, a gente treina ele com palavras pra prever a próxima, pra fazer uma tradução, mas a gente passa basicamente textos da internet inteira, então ele vai pegar o Wikipedia inteira, pega livros, blogs, enfim, literalmente a internet inteira e também textos criados por IA hoje em dia, cada vez mais isso de dados sintéticos, e ele joga lá pro modelo treinar. Só que pra isso foi desenvolvido anos atrás ali esse conceito de embeddings, que é um jeito de tentar pegar ali de uma maneira numérica o significado semântico daquela palavra ou daquele token, que é um outro termo que a gente usa aí que é meio que uma sub-palavra, mas vamos usar a palavra aqui que dá pra entender melhor. Mas, enfim, e aí quando a gente olha aqui, por exemplo, a palavra rei, ela tá muito próxima, semanticamente, em significado, da palavra rainha. São palavras que fazem sentido juntas. Ou então aqui a palavra carro tá próxima da palavra táxi, da palavra ônibus, da palavra trem, né, porque são coisas ali, meios de transporte, a gente pode dizer, mas elas estão muito longe da palavra focinho, por exemplo, que não tem nada a ver ou a acariciar, ou escola, enfim, essa é a ideia do embedding. E se a gente for aqui até pra próxima página, olhando aqui, por exemplo, o embedding da palavra avião, isso aqui é uma coisa inventada, né, mas, ó, ele tá muito próximo, vamos dizer que 100% seria a mesma palavra, e ele tá, ó, 95% de proximidade da palavra voar. Ou 78% da palavra trem, que também é meio de transporte, mas tá um pouco mais longe do que voar. E, ó, 1.5% da palavra focinho, tá longe, não faz o menor sentido, né? Então se a gente olhar o que é o embedding na prática, ele vai ter mais ou menos essa carinha aqui. Exato, porque a máquina, ela não sabe o que significa cachorro, não sabe o que significa capivara, mas ela entende números, então qualquer distância entre essas palavras, qualquer probabilidade delas estarem ali pertinho. Exato. E acho que uma coisa até também super legal desses embeddings é que quando a pessoa que criou elas percebeu ao criar que você consegue fazer operações matemáticas, como essa de distância, mas, por exemplo, você pega a palavra rei, ela tá na distância tal da palavra homem. Se você pega rei e subtrai homem, mas adiciona mulher, o resultado vai estar muito próximo do embedding, do numérico, o valor numérico, da palavra rainha. Então você consegue fazer isso não só, por exemplo, com isso, mas também, ó, por exemplo, verbos, né? O andando, você tira o andei e bota o passado, nos que seria o caso, o cozinhando, você vai chegar em cozinhei, então você consegue fazer essas operações matemáticas com significados semânticos das palavras. Então essa é a grande vantagem dos embeddings, que é o que a gente vai usar aqui agora, né? Exato. E a gente tava fazendo aqui pra palavras, né, pra tokens, só que no nosso caso a gente vai fazer pro chunk inteiro, vai fazer o embedding não só de cada palavra, mas o embedding de tudo isso daqui, né, de todo esse chunk inteiro. Que aí é o que a gente chama de sentença, né, o sentença embed. A gente pode fazer o embed só da palavra, mas pode fazer o da sentença como um todo. E eu acho que vale só reforçar, você explicou muito bem, Fabrício, mas é só adicionar a palavra vetor aqui, porque no final é um vetor. O embedding, ele pode ser muito, diretamente relacionado com o vetor, por isso que é possível fazer operações matemáticas, né? Você faz na aritmética tradicional, você faz esse cálculo vetorial, distância entre vetores. Então como uma palavra ou uma sentença vira um vetor, você consegue computacionalmente fazer esse cálculo de similaridade entre as palavras que por trás matematicamente são vetores, né? Perfeito. E quando você vai fazer isso no código, usando modelos específicos que fazem embeddings, por exemplo, você consegue definir a dimensão do vetor de cada palavra ou de cada sentença, né? Então você pode definir, por exemplo, aqui é um aleatório que por acaso tem 20 dimensões, que seria pequeno até. Você pode ter um de 300, que é um padrão que pode ser definido, mas você pode ter de 1000, pode ter de 128, você define isso de acordo com padrões do mercado, e aí você pode ter, por exemplo, um embedding de 300 dimensões para uma palavra ou para uma sentença, ou para um parágrafo inteiro, ou para um livro inteiro. Aí se você fizer para um livro inteiro só em 300 dimensões, é provável que ele perca alguma informação no meio do caminho. Por isso que a gente separa em trechos, em chunks menores, né? E você falou de modelo, né? Perdoe. Você falou de modelo e isso é importante reforçar para o pessoal que a gente definiu na aula anterior o modelo, o de M.I. 2.5 flash. Só que a gente precisa agora realizar os embeddings, então a gente vai precisar de um modelo específico para embeddings. Não é o mesmo do de M.I. que gera as coisas, né? Não. Esse aqui é um modelo para construir um vetor a partir de algo que você manda para ele. Exatamente. Isso é super importante, muito, muito importante, Carol. E eu vou só reforçar. Porque o LLM, ele usa o conceito de embedding, mas ele não foi treinado para gerar embeddings. Ele é treinado para, a partir de uma entrada, gerar uma saída. Apesar dele usar todo o conceito de embeddings, porque é assim que ele gera a próxima palavra. Agora tem modelos específicos que são para, a partir de uma entrada de texto, gerar o embedding daquele texto. Então tem essa diferença e é um pouco disso que a gente vai precisar usar agora. Perfeito. E nesse momento, o que a gente precisa? A gente precisa importar exatamente essa biblioteca, que é o Google Generative AI Embedding. Eu vou repetir. Mas é from LinkedIn, Google Gene AI, que a gente cortou lá em cima. A gente cortou lá em cima. A gente instalou aqui. Ah não, a gente não instalou ela. Na verdade nós instalamos na aula passada. Na aula 1, nós instalamos ela. Linking, Google Gene AI, nessa. Isso. É, lá na aula 1 a gente instalou. Então, from LinkedIn, Google Gene AI, import. Google Generative AI Embeddings. Isso. E a gente vai fazer aquela mesma declaração que a gente fez para o LLM na aula anterior. A gente vai fazer agora para embeddings. Então a gente pode criar uma variávelzinha chamada embeddings. Eu só estou notando que ele está reclamando, mas eu acho que é porque agora eu estou com um dashzinho aqui com o ifen. E na hora de instalar é o ifen, mas para importar o underline. Aqui é a underline. Aí, para de reclamar. Então acho que chamar de embeddings, a variável que você fez, é o ifen. Então, igual a Google Generative AI Embeddings. Esse carinho aqui que a gente importou. Abre parênteses. Aí a gente vai ter model igual. Um dos modelos que a gente pode usar, que a gente vai usar aqui, é o Gemini Embeddings 001. Então, esse aí, entre aspas. Models. Barra. Gemini. Tracinho Embedding. Tracinho Embeddings. AI. Tracinho Embedding. Tracinho 001. Embedding ou Embeddings? Ding. Perfeito. 001. Tem um aqui. Vírgula. Não, em cima. Isso, vírgula. E você vai ter que passar a sua API key que você está utilizando. A mesma lá que a gente estava passando. API key. Igual a Google. Ele já está completando. Google API key. Ele está chamando aqui. Google API key. Recebe Google API key. Aquela mesma variável que a gente... Essa daqui, olha. A gente pegou lá em cima. Vou dar um Ctrl C. O que a gente fez foi muito parecido em carregar o modelo de LLM, que foi o Gemini Flash, mas agora é o Gemini Embeddings. E a gente não precisa passar, por exemplo, não tem a variável temperatura. Porque temperatura é específico para o modelo de geração. Nesse caso, tem um Embedding e não tem essa variável, por exemplo. Perfeito. E agora, Vinny? Legal. Perfeito. A gente carregou os Embeddings. Porque é o que a gente precisa. A gente precisa ter a representação numérica das sentenças que a gente quebrou em chunk. Legal. Tendo isso, o que a gente vai fazer agora é usar esses Embeddings para fazer o cálculo de similaridade. Para entender o que está dado uma entrada, que chunk eu tenho que recuperar, como que eu faço isso. Então, a primeira coisa que nós vamos fazer é importar aquela biblioteca que nós já instalamos, que é a FAIS, que vai ser a biblioteca que vai ajudar a gente a fazer essa busca eficiente. Porque a gente tem o modelo de Embedding nos ajudou a fazer de forma eficiente o melhor Embedding. A conversão do texto para o Embedding. A conversão do texto para o Embedding. Mas agora eu tenho dois Embeddings, dois vetores. Eles são similares ou não? Então, é uma outra biblioteca que ajuda a fazer isso. Então, vamos lá. Essa biblioteca é do Facebook. Que é a Facebook AI Simularity Search. É muito utilizada e ela é open source, então todo mundo utiliza. Você pode utilizar, ver o código lá, está tudo certo. Então vamos lá. From lungchain.com.br. VectorStores. VectorStores, os armazéns de vetores. Vetores, perfeito. Import, tudo maiúsculo. Faz, F-I-S-S, que vem do nome que a Carol falou. Primeiro F de Facebook. Legal. E agora a gente vai então, primeiro a gente importou. Agora nós vamos criar de fato esse VectorStore para ir armazenando. Então, vamos dar o nome de VectorStore mesmo, que é um nome padrão, tudo junto. Igual pais, que é o que a gente acabou de importar. Ponto. Da onde que eu vou importar os chunks que nós quebramos agora? Então, from, underlining, documents. Então, eles vão vir de algum documento. Documents. Pais, ponto, from, underlining, documents. Exato. E agora eu vou passar os parâmetros que é os chunks que nós quebramos. Então, chunks, que nós demos o nome de chunk lá atrás. Todos eles. Chunks é a técnica, mas nós demos o nome de chunk lá. Então, tá bem. Que são legal. Vírgula, embeddings. Tá. Que é essa variável aqui. Que é a variável que nós criamos, que é o modelo que vai pegar o chunk, converter embedding e salvar no nosso... Usando o modelo que nós retamos anteriormente. E vai salvar isso no nosso VectorStore. Perfeito. O final do VectorStore vai ser todos os chunks já convertidos para vetores de embeddings, basicamente. Usando o modelo de M9 embedding 001. E o faz para achar as comparações entre eles. Exatamente. Perfeito. Posso rodar essa cela ou mais alguma coisa? Agora, beleza. A gente criou esse VectorStore. Agora vai ser importante a gente criar uma forma de... Lembra, o que é o R do HAG? O retrieval. Retrieval. Que é a recuperação. Eu só sei. Agora eu tenho que conseguir recuperar isso. Então é a última parte que a gente cria aqui para poder usar. Então vamos lá. Porque a gente tem que setar assim, como que a gente vai fazer essa pesquisa? Então qual será o nosso limite para definir se há uma similaridade ou não? E quando a gente fizer essa busca, qual que vai ser a quantidade de retornos dessa busca que a gente vai aceitar? Será que vão ser 100, 10, 5, 4? A gente tem que setar esses parâmetros. E a gente está falando muito de busca, de recuperação, mas a gente faltou mencionar que o que a gente vai buscar é exatamente a pergunta do usuário. Exatamente. Então o usuário vai perguntar, posso reembolsar a internet, como a gente fez no teste? O que vai acontecer? Essa pergunta vai ser convertida também em um vetor de embeddings. Exato. E a gente vai comparar lá no VectorStore com todas as coisas dos documentos, dos PDFs que a gente subiu. E ali a gente vai ver numericamente qual vetor está mais próximo de qual, dessa pergunta. E aí pode dar, como a Karol falou, pode dar 100 resultados e aí vai estar a distância, sei lá, de 0.8, 0.7. A gente pode listar, tipo, eu quero que retorne só o primeiro mais próximo, os três mais próximos e eu faço um bem bolado ali no meio. Muito parecido com o exemplo do seu livro que você mostrou, que tinha lá a palavra avião e tinha um número que até você puder abrir novamente, ó. Então, olha lá, eu tenho várias palavras, vouar, trem, ônibus, focinho, cada uma com um número de similaridade. Com o próximo eles estão. Com o próximo eles estão da palavra principal. Então avião aqui seria similar à entrada do usuário e de todos os chunks que eu tenho qual é a similaridade. E aí eu vou tentar retornar sempre os que têm mais similaridade. Perfeito. Eu vou ali o primeiro chunk, por exemplo, trem, o segundo, etc. Exatamente. Maravilha. Então, voltando lá. Vamos voltar lá. Chegou essa parte de fazer a busca, né? Vamos fazer esse componente que é o retrieval. Então vamos lá. Retriever. Retriever. É o buscador. É o buscador. Onde que ele vai buscar? No nosso vector store que a gente acabou de criar. S underline. Desculpa, vector store é onde foi? Ponto. Ele está sugerindo. Ele está sugerindo, mas vamos ver se é isso mesmo, né? O que a gente vai fazer dentro do as retriever? As retriever. E aí eu vou apagar aqui para a gente fazer juntos. Mas as retriever, o que vai ter dentro do as retriever? Então a gente vai colocar o tipo de busca, acho que a Carol falou um pouco, o tipo de busca e qual que vai ser o threshold que a gente quer recuperar. O limite. O limite, o tamanho, etc. Esses parâmetros da busca, tanto a nota de corte da similaridade, você pode deixar uma nota de corte muito grande e envolver muitas coisas que estão bem distantes nesse espaço vetorial. E a quantidade que pode ser 4, 10, a gente vai decidir. Então vamos lá. Primeiro o que nós vamos criar vai ser o search type. Então pode dar um enter. Search. Pode colocar na frente também. Underline. Dá um control, ele não achou nada. Search type. Search type. Perfeito. Igual. Igual. Entre parênteses. Similarity. Um dia eu vou ficar com o sotaque tão bonito quanto o da Carol e do Fabrício, mas o inglês ainda não está. Underline score. Underline threshold. Essa palavra é complicada. Threshold. Threshold. Então esse é o tipo que nós estamos usando. Então vamos fazer a similaridade por esse score que ele vai gerar, que a própria biblioteca gera. Então a gente vai fazer a mesma coisa. Então vamos criar agora. Pode dar vírgula. O search. Eu não sei como pronunciar, mas é o coworks. Então agora a gente vai pôr igual. É dois pontos. É dois pontos. Dois pontos. Aí a nota aqui vai ser normalizada, então não vai ser igual a gente estava vendo no livro do Fabrício. Ah, então calma. Então ele vai ter que dar um igual e depois abrir uma chave, porque eu vou ter que passar um dicionário pra ele. Isso. Então nós vamos pôr o score threshold, que é o score que vai ser criado. Exato. Aí pode fechar parênt. Aspas, dois pontos. Agora sim, dois pontos. Agora vamos mencionar. Vou botar aqui até na linha de baixo pra ficar mais claro. O primeiro foi o search type, que a gente definiu tipo. Aí vírgula. Aí o próximo a gente vai passar os argumentos de busca. Exatamente. Que é um dicionáriozinho. Então score threshold, que vai ser. Por exemplo, 0.3. É, o padrão é 0.3, mas é até legal depois o pessoal faz... A gente pode até fazer aqui alguns testes, mas 0.3 é o padrão, mas vale colocar um 0.7 depois pra ver essa diferença. Aí vai de 0 a 1, né? Pra ver qual que é o limite de pontuação que é aceitável. Exato. Pra ele buscar um retorno ali. Então quanto maior o limite, mais restritivo você tá sendo. Se colocar um só vai ser se for exatamente igual. Quanto menor, mais amplo. Então tudo tem um ponto positivo e negativo sobre isso. E a quantidade de documentos. Ou de chunks na verdade. Desculpa, a quantidade de chunks que a gente quer retornar. A gente também pode pôr um tamanho... Um K aí a gente pôr vírgula, né? E o tamanho que é o K entre parênteses. É... Desculpa, entre aspas. É 4. Que é também um valor padrão pra testar. E aí perfeito. Podemos já rodar. Isso. Beleza. E aí isso aqui não fez nada, né? Ele simplesmente... A gente tá definindo aqui, olha... Os padrões de busca. Como que a gente vai fazer essa busca de similaridade dos chunks que a gente transformou em embedding. Quando você for fazer, né? Respeita isso aqui. Exatamente. Isso. Beleza. Então mais código. Já é. Criamos... Então temos o modelo que gera embedding, temos o nosso VectorStore, temos os chunks criados. E agora a gente tem que ir pro próximo passo. E antes disso, eu vou aproveitar uma palavra que você falou pra você de casa. Pra ver se você tá acompanhando mesmo essa imersão. Lembrando que pra você ganhar o certificado, você tem as palavras-chaves que estão em algum momento durante as aulas aqui. E nessa aula dois, a palavra-chave, que vai estar escrito aqui também na tela, é chunks. A gente vai usar bastante, né? No plural, com S no final. Então chunks é a palavra-chave dessa aula dois. Continuemos. Legal. Então da mesma forma que nós fizemos também um pouco na aula passada, agora a gente tem que criar o nosso prompt. Que vai definir exatamente como que vai ser... A gente vai explicar pro nosso agente o que ele vai fazer, como que ele vai funcionar. Isso que a gente definiu agora, a gente foi fazendo toda a estrutura pro nosso agente. Só que o nosso agente nem sabe quem ele é ainda. Então a gente vai voltar na definição do prompt de sistema. Desse cara aqui, né? Desse cara que vai estar como parte do nosso agente. Exatamente. Quais bibliotecas que vamos usar? Agora a gente vai voltar pro Lang chain, que é o que a gente já tava utilizando na aula passada. Então pra isso, pra gente fazer a definição do prompt de sistema, a gente vai começar importando... Esse é exatamente igual ao que nós fizemos na aula passada. Exatamente. Aí na próxima linha a gente vai fazer um outro import. Então também do Lang chain, pra um Lang chain ponto... Não é do core, né? É só do Lang chain nesse caso. Isso. Lang chain ponto chains ponto combine underline documents. Ele quer me completar? Eu não quero que você complete. E a gente vai fazer um import que é o create underline stuff. Esse que ele tá sugerindo aqui, né? Create, tudo minúsculo. Create underline stuff, underline documents, underline chain. Basicamente. Exatamente. Porque agora o que a gente vai fazer? A gente tá fazendo essa cadeia pro nosso agente, né? Essa cadeia do Lang chain. Então essa funçãozinha que a gente tá importando, ela vai pegar todos esses documentos e vai utilizar como contexto pro nosso prompt. Tá. Então vamos criar nosso prompt. Vamos. Como parecido com o que nós fizemos na aula passada. Podemos dar um nome, como estamos fazendo o rag, vamos dar o nome de prompt rag, que prompt underline rag. Pront underline rag, beleza. Igual. Vamos chamar, como ele sugeriu ali, o chat prompt template, que nós acabamos de trazer. Ponto. From underline messages. Perfeito. Aí entre parênteses. Que é um método, né? É um método. Aí a gente abre... Cochetes. Cochetes. Então vai ser uma... é, como a gente fez no outro, né? Porque aqui é mais lista. Exatamente. Exatamente. Exatamente o como a gente fez no outro. E aí vamos passar... a gente vai passar um prompt explicando um pouco do que ele é. Então podemos colocar alguma coisa assim, ó. Systema ou system, né? Você é um assistente de políticas internas. Depois do cochetes eu já abro a... a aspa pra começar a digital? Abro uma... Cochetes. Parênteses de novo. Parênteses, desculpa. Parênteses de novo. E aí tudo... tudo que a gente passar a gente vai colocar entre aspas, né? A gente pode usar um padrãozinho que nós criamos? Sim. Eu vou copiar lá que é mais fácil pra não ficar perdendo tempo de vocês aqui também. E esse texto também vai estar aqui embaixo dessa aula pra você copiar também. E com atividade o pessoal pode experimentar, mudar o que tá nesse textinho. Então é um padrãozinho que a gente criou. Mas você pode inventar um nome pra sua empresa de desenvolvimento. Pode inventar uma resposta do agente. Cabe experimentação. Boa. Então, ó, no caso a gente tá passando uma lista aqui de tuplas na verdade, né? Porque tá entre parênteses. Então a primeira parte da tupla seria esse... o sistema. Você é o sistema. Você falando você é o sistema e aqui o vírgula. E aí a resposta, né? Tipo, a segunda parte dessa primeira tupla. Você é um assistente de políticas internas da empresa Carrara de Desenvolvimento, que a gente tava usando desde o começo. Responda somente com base no contexto fornecido. Isso é uma coisa muito importante quando a gente trabalha com o Rague. Pra ele não responder com base na inteligência dele, mas só com base nos PDFs ou nos chunks aqui, né? Porque como você já comentou na aula passada, o modelo tem o conhecimento dele e a gente tá adicionando o chunk. Se a gente não reforçar, ele vai misturar as duas coisas. E aqui a gente tá falando, não, eu quero que você foque nos dados que nós passamos. Porque a gente quer reduzir o máximo de alucinação, ou coisa assim. E lembrar da temperatura também. Porque aqui quando a gente definiu o Gemini, a gente escolheu a temperatura. De zero, né? Exatamente. Então a gente tá falando, eu quero ficar nesse contexto mesmo. Perfeito. E eu até botei, se não houver base suficiente nesse contexto, responda apenas não sei. Porque o modelo clássico, se eu falar no Gemini, eu acho que eu nunca tive essa resposta, não sei. Não sei, é. Então ele sempre vai tentar responder alguma coisa. Mas aqui a gente não quer que ele tente. Vai também reduzir mais uma vez a alucinação. Ou mesmo ele poderia responder que ele não sabe, mas de uma maneira um pouco mais ampla. Então não tenho essa informação. Aí na segunda vez que você vai dar o play, não obtive conhecimento sobre isso. Então eu tentei aqui falar pra ele responder apenas não sei e exatamente o texto não sei ali. Perfeito. Pra sempre ser esse padrão. Então esse foi o primeiro elemento da nossa lista. O elemento da posição zero, essa tuplinha aqui. Que é a parte do... Da mensagem do sistema, que a gente tem criado na aula passada. Tem a mensagem do sistema e a mensagem do humano do usuário. Exato. E aí a mensagem do humano é o segundo. Esse daqui é o elemento zero da nossa lista. Esse aqui é o elemento um da nossa lista, que é o human. Você é humano. E aí a pergunta, e a gente tá até passando aqui, o input, uma variável, tá entre chavinhas. Então a gente vai conectar ali esse input, uma variável que vai ser a entrada. Isso. E também o contexto que vai ser os chunks que foram encontrados ali. Perfeito. Que tem sentido com essa pergunta. A pergunta no caso vai ser o input. Poderia até trocar aqui de input pra pergunta. Ou enfim, você pode trocar do jeito que você quiser pra ficar mais claro. Legal. Eu não entendi ainda como é que a gente vai conectar esse cara aqui, essa variável que tá dentro do from messages, que se chama input, se chama pergunta, com uma variável que a gente vai dar pra ele chamado input, chamado pergunta. A gente vai ter que conectar essas coisas. Perfeito. Vamos fazer essa conexão agora usando o lang chain. A gente vai criar esse chain, essa cadeia pra gerar a resposta final combinando tudo isso. Então logo na sequência a gente coloca. Vou deixar aqui como input por enquanto, como base, mas depois a gente muda se a gente quiser. Pode ser. Então a gente tem o document chain, que é a nossa cadeia aqui de documentos, é igual create underline stuff, que a gente também já importou lá em cima. É o que a gente importou, né? Vou só copiar e colar ele aqui. Ele é um método então, abre e fecha e pare em si. E aí o que nós vamos passar? O LLM, que é o nosso modelo que nós já setamos. Lá em cima, não é o LLM triagem, acho que era o nome. Lá não, lá um, né? A gente setou. É, a gente criou dois. A gente criou o LLM triagem, que foi o primeiro, mas no primeiro teste a gente criou o LLM também. Volta mais um pouquinho que você vai ver o nosso LLM. E aqui no final vai dar no mesmo, gente. Vai dar no mesmo nesse caso. É que é o mesmo modelo, né? É que a gente tinha deixado esse LLM só pra vocês fazerem testes aí, né? Então vai mudando a temperatura, faz o que quiser. E se o pessoal tiver em um outro notebook, pode fazer a mesma criação. Fica aí o dever de casa de criar de novo, né? Perfeito. A declaração. Então aí lá eu posso colocar qualquer um dos dois no caso, né? Pode ser o LLM ou LLM triagem. Vou botar o triagem pra manter o padrão, mas enfim. Perfeito. E o prompt rag, que foi o que nós prompt, underline rag, que foi o que nós acabamos de criar o rag aqui. Tá, então esse método aqui ele mesmo já diz o que ele fala, né? Então create stuff, cria coisas, documents chain, uma cadeia de documentos. E quando a gente bota o mouse aí em cima ele até fala, ó. Cria uma cadeia para passar uma lista de documentos para um modelo. Perfeito. Que são a lista que nós criamos anteriormente. A lista de documentos, né? A lista de documentos. Perfeito. Posso dar rodada de blocos aqui? Pode rodar, vamos embora. Beleza, agora a gente meio que conectou as coisas, mas agora a gente vai ter que realmente fazer... Criar a função com a qual a gente vai ter que mandar pergunta, né? Pro nosso rag e aí ele vai fazer a busca e tudo mais e retornar as respostas pra gente, as que tenham mais similaridade semântica, né? Similaridade de significado. Boa, vamos então criar essa função que a gente... Seria a nossa função principal agora. Que vai fazer toda essa conexão que você falou. Então vamos lá. Def, né? Para criar a nossa função. A gente pode colocar um nome... Ah, perguntar política. Perguntar, né? Política, underline rag, que é a nossa questão, né? E aí entre parênteses nós vamos passar então uma pergunta. Sim. Dois pontos. Str. A gente está mandando uma string pra ele, né? A gente manda uma string e dentro de um dicionário. A gente retorna um dicionário. Retorna um dicionário. Perfeito. Então essa é a primeira parte que estamos definindo a nossa função. E dentro dela... Então o que nós vamos fazer? Primeira coisa é usar o retriever pra pegar os nossos documentos mais importantes, ou mais relevantes, os chunks, né? Mais relevantes dentro da nossa vector store. Perfeito. A gente tem que fazer uma invocação e passar a pergunta. É, sim. Ele é o cara então que vai fazer a comparação de quão distante eles estão. Porque ele é o cara que tem a nossa vector store. Então vamos lá. Pegar o retriever daqui. Então a gente vai pegar os documentos relacionados. Então docs, underline real, alguma coisa de relacionados. Docs relacionados, vou chamar. Docs relacionados é igual ao retriever, que ele vai fazer esse retorno, Retorno.invoke e vai passar a pergunta. Só pergunta. Essa que a gente recebeu aqui como parâmetro. Perfeito. Feito isso, a gente pode fazer uma verificação simples pra entender se o documento que está sendo retornado é relevante ou não. Então essa seria a próxima etapa aqui da nossa função. Então a gente pode fazer um if. Vamos colocar not. Então if not docs, underline real. O relacionado. O relacionado, perfeito. Foi o que a gente tira dois pontos. E nós vamos fazer um retorno se nenhum documento for encontrado. A gente vai retornar esse dicionário indicando que o contexto não foi encontrado. Então a gente põe return, que é o de retorno. Abre chaves. Entre parentes, answer. Answer, resposta em inglês. Dois pontos fora do... Isso, perfeito. Ele até sugeriu ali um não sei. Vamos seguir o não sei. Que era a mesma coisa que a gente tinha falado lá no prompt do RAB, mas faz sentido. Como nós fizemos na aula 1, falamos no prompt que tinha que ter as categorias, mas depois a gente tem que colocar no código. É mais ou menos o que a gente está fazendo aqui. Então não sei, vírgula, depois não sei mais coisas. A gente pode colocar algum tipo de citação na frente. Põe citações também entre... Entre aspas, desculpa. Dois pontos, abre e fecha a chave. Porque não tem nenhuma citação, ele não encontrou. Encontrou, beleza. E o contexto que não foi encontrado. Então o contexto encontrado. Contexto underline encontrado. Dois pontos e fora. Falsa, false. Porque não foi encontrado. Então a resposta é não sei, não tem citação, porque não tem chunks nenhum. E o contexto foi encontrado, não, falso. Perfeito. Então o que a gente está fazendo é... Vamos definir essa função, perguntar política rag. E aí sempre a gente vai retornar um dicionário com esses três chaves. Answer ou resposta, citações e contexto encontrado. Perfeito. E aí a resposta, se tiver, se ele encontrar alguma coisa, ele vai dar a resposta gerada pelo LLM com base nessas citações. Vai retornar citações também e vai retornar se teve ou não teve. E depois a gente trabalha com isso. Nesse caso que ele não encontrou nada, não tem nenhum documento relacionado. Ele já, ele meio que pré-define isso daqui pra gente não ter que pedir pro LLM gerar uma resposta de não sei. Então já mata ali antes do negócio. Mas e se ele encontrar? Aí se ele encontrar é o caso feliz. Exatamente. Então a gente, assim como a gente definiu answer no dicionário aqui, se ele não encontrar, ele vai ter que definir o answer se ele encontrar. Então answer vai ser igual a document underline chain. Que é esse cara que a gente definiu aqui em cima, né? A gente passou nosso LLM e o nosso prompt do system prompt do rag. Ah, aqui foi o que eu perguntei pra vocês. Onde que eu ia conectar essa variável input e essa variável context que a gente colocou lá no prompt? Exatamente agora na hora de gerar resposta. Faz sentido. Pra gerar resposta eu vou conectar, tenho que conectar tudo pra gerar resposta. Faz sentido. Exatamente agora. Beleza. Então a gente vai invocar. Ponto invoke, né? Ponto invoke. Abre parênteses. Abre chaves. Então a gente vai passar entre aspas input. Que é esse nosso cara aqui que eu até falei, né? Se você pergunta, poderia ser input. A gente chamou de input aqui. Dois pontos, pergunta, vírgula e o contexto. Que são aqui os documentos que a gente passou. Perfeito. E a pergunta no caso seria a pergunta que a gente recebeu aqui na variável, né? E aí o contexto vai ser o context que também a gente colocou aqui, né? Entre chaves. Basicamente eu tô passando pra ele. Eu sou humano. Usa a pergunta, dois pontos. Essa pergunta que tá dentro da variável input. E aí o contexto são esses chunks que estão dentro dessa variável context. E aí eu tô fazendo essa conexão aqui embaixo. Input recebe essa pergunta e context vai receber, eu imagino que, os chunks que foram encontrados lá nos documentos relacionados. Exatamente. São os docs relacionados aqui. Perfeito. Maravilha, faz sentido. Entendi. Então vamos continuar aqui. Bom, feito isso, então a gente tem como que ele vai formar essa resposta, né? Então pra formar a resposta a gente passa pergunta e os documentos relacionados. Mas uma vez que eu tenho perguntas e documentos relacionados, quem que vai moer tudo isso pra gerar? Então eu tenho que chamar agora o LLM, certo? Então agora nós vamos fazer... Primeiro a gente vai limpar a resposta que foi gerada pelo LLM. Pra não ter nenhum lixo ali, porque a gente vai sempre estar passando bastante coisa. Então pode criar um TXT. TXT, texto, qualquer coisa assim. É, texto, qualquer coisa. Abre parentes, answer, que é a resposta. Nossa variável aqui. Aí a gente vai colocar um or de ou. E aí abre e fecha aspas duplas, por favor. E aí nós vamos pôr ponto, strip e abre e fecha parentes. Tá. Então isso a gente vai limpar a resposta gerada pelo LLM usando a função de cleantext mesmo. É de remover as coisas, esses espaços, essas coisas. Perfeito. Então beleza. Agora a gente vai verificar se a resposta limpa, dado toda essa limpeza, se ela é o não sei. Porque lembra que a gente quebrou. É o não sei ou eu tenho o contexto, tem as coisas. Então a gente vai fazer essa verificação na sequência. Legal que a gente tá fazendo meio que duas vezes, né? Primeiro a gente faz aqui, ó. Se não tem um documento relacionado, já responde não sei. E depois a gente tá fazendo de novo aqui, ó. Se quando você bater no LLM ele responder não sei, que é o que a gente tinha pedido lá no prompt inicial, lá em prompt triagem, né? Responda não sei e tudo mais. No prompt triagem não, perdão. No prompt rag, responda apenas não sei, aí a gente tá fazendo muito seguro o nosso rag. Exato. Para ele ficar o mais seguro possível, né? Perfeito. Então vamos lá. Então vamos fazer um if aqui para fazer essa verificação, né? if. O txt que nós acabamos de criar. Um ponto rstrip. Aí abre parentes, aspas dupla. Aí a gente vai por um ponto, exclamação e interrogação. É, tipo, então é pega ali esse texto, remove do final o ponto, remove qualquer pontuação. Tira tudo. E aí a gente põe igual igual não sei. Não sei, entre aspas. Sem o ponto, no caso. Sem o ponto. Porque a gente removeu tudo. A gente removeu. Removeu, limpo, removeu tudo. É igual não sei? Se de fato a resposta for não sei, que é o que a gente tá olhando, a gente vai fazer o retorno. Esse mesmo de antes, né? Vamos fazer um answer, é um return na verdade, né? E vamos dar o mesmo retorno que a gente já tinha criado no if, que é, que é, answer não sei, citação vazia e contexto encontrado falso. Perfeito. Esse também é um caso triste, vamos dizer assim. Agora já é o caso feliz? Agora é o caso feliz. Agora eu tô feliz, esse foi o caso triste, agora sim. Porque o que a gente... Acho que isso é bem importante a gente frisar pra quem tá acompanhando entender. Por que a gente tem que fazer duas vezes? Porque a gente tem meio que dois processos acontecendo. O primeiro é, eu busquei chunk. Tem alguma coisa, tem algum contexto? Não encontrei nada. Quando eu não encontro nada, eu posso mesmo assim chamar o modelo sem nada. E aí ele vai subverter o que a gente quer. Ele vai usar o conhecimento dele pra gerar resposta. Isso pode responder, é. Pode responder alguma coisa, mas a gente só quer responder com base nos chunks. Então por isso que a gente faz essa dupla verificação. Se tem contexto e se o modelo respondeu não sei. Então pra de fato ter a total garantia. Perfeito. Bom, eu quero ficar feliz agora e vamos pro caso feliz. Que seria basicamente já retornar essas pessoas, essas variáveis aqui. Perdão? Essas chaves no caso. Só que o answer não vai ser não sei, eu imagino. Vai ser o texto que a gente limpou aqui. Se não for não sei, aí ele vai se retornar. Se for não sei, ele vai fazer toda a limpeza ali e a exceção. Se não for, vai retornar ele. E tá tudo caso agora, porque no cenário feliz a gente vai retornar o answer txt. As citações do docs hell relacionados. Docs relacionados lá que a gente botou. E ele encontrou o contexto. Contesto encontrado, sim. True. Perfeito. Essa aqui é uma função super simples. Simples, né? Se você pensar depois da gente criar. Mas ela é basicamente como eu falei na outra aula. Que tinha uma função que era mais importante. Essa talvez seja mais importante. Que é onde conecta tudo que a gente fez até agora nessa aula. Perfeito. Essa é a nossa menha aqui. Vamos pensar assim, né? Exatamente. E agora é só testar. Agora é testar. Maravilha. Bora testar. Vamos lá. Vou chamar aqui testes. Testes dois, talvez. Na verdade a gente podia até copiar o que a gente fez na aula postada. Verdade. Poderia ter mais fácil. Podia deixar lá, mas eu quero ficar próximo aqui. Pra gente poder ler. Não ficar subindo e descendo. Ler, mexer, fazer umas mudanças. Legal. Exato. Então eu redefinir aqui. E aí pra chamar eles eu vou ter que fazer de novo aquele forzinho basicamente. Isso. Vamos fazer um for pra ir chamando, como é uma lista de coisas, né? Chamando uma uma. Então vamos lá. For. N ou alguma... Vamos pôr N. Ele já tá sugerindo aqui. Ou mensagem. Ele tá sugerindo igual ele tinha feito antes, né? Então, message test em testes. Beleza. Perfeito. Eu posso manter isso aqui de pergunta e resposta ou melhor não? Agora talvez quebraria porque na anterior a gente não fez rag. Verdade. A gente só batia no modelo. Então agora vale quebrar em alguns passos. Perfeito. Então o primeiro eu vou gerar uma resposta. Então pode colocar aí. Primeiro a variável. A variável resposta que nós vamos gerar. Só variável. Antes de printar. Assim, claro. E tem que envocar a função. Obviamente. Tem que chamar essa função aqui e perguntar pra gente que é errado. Beleza. E aí nós vamos passar o... E passa o mensagem test. Passando o teste. Uma pergunta, né? Que ela recebe uma pergunta. Então eu vou passar cada uma dessas perguntas aqui do teste. Maravilha. E aí vai ficar armazenado aqui em resposta. Agora sem printar. Printamos a resposta. Posso fazer bonitinho lá aquele negócio de resposta? Ah, é. Dos pontos. Podemos printar pergunta. Também se quiser pergunta, depois a resposta. Perfeito. Vamos fazer. Vamos fazer bonito. Eu gosto de beleza. Aqui, ó. Pergunta vai ser um message test. A resposta vai ser a resposta que ele obteve lá. Mas a gente quer printar também se ele teve... É que a resposta, na verdade... A gente pode passar por aqui. Tem o answer. Que é o answer que nós criamos. Ele retorna aqui um dicionário. Que é answer citações e contexto encontrado. Então vai ser resposta... Abre cochetes. Cochetes, né? Na chave... Aspas, insert. Exatamente. Perfeito. Isso aí. Isso aí. E aí a gente quer também, porque ele poderia retornar citações. Isso. Se existe, então a gente pode imprimir também. A gente quer saber quais foram os chunks usados. Porque é legal você saber a resposta, mas como a gente está construindo o sistema, saber quais foram os chunks que ele usou para criar aquela resposta que a gente já tem de saber se está indo bem ou não. Então podemos fazer isso. Perfeito. Então vou botar aqui um print citações ou algo nesse sentido, né? Vou até botar tudo em maiúsculo para ficar mais claro o que é a pergunta e o que é a resposta e tudo mais. E aqui também maiúsculo citações, dois pontos. E aqui embaixo eu vou dar um outro print, em vez de fazer um f-string, depois eu vou fazer separadinho. Vai ser exatamente isso. Resposta na chave citações. Igual a gente fez aqui resposta na chave answer, aqui resposta na chave citações. Esse cara aqui, o contexto encontrado true, a gente pode usar ele aqui para um if talvez, né? É, se a gente quiser. Poderia, né? Então se resposta no contexto encontrado. Como a gente está filtrando... Bom, vamos fazer o if. Vai ficar mais bonito. Contesto encontrado. Se existe resposta no contexto encontrado, ou seja, isso aqui vai retornar true, aí eu printo as citações. Isso. Se não, aí eu não pinto nada. Não pinta nada, ele não vai entrar no if aí. Exato. Perfeito. Então, assim a gente já consegue testar? Podemos, conseguimos. Acredito que sim, vamos lá. Vamos dar o play. Reta final que dá, olha que beleza. Aí está dando certo. Pergunta, posso reembolsar a internet? Resposta, sim, a internet para o home office é reembolsável via subsídio em salde até R$100 mediante nota fiscal nominal. E aqui citações, ele botou aquela coisa que a gente tinha visto antes lá, o document, com um monte de coisa, mas vamos tentar achar que em algum lugar vai ter política de reembolso e tudo mais. Indo mais para dentro, estão reembolsáveis, transporte, tudo mais. Que se a gente for olhar lá na política de reembolsos, internet para home office, reembolsável via subsídio em salde até R$100 conforme política, blá, blá, blá. E até até aqui ele botou reembolso que requer nota fiscal e tudo mais. Então meio que ele já conectou. Conectou tudo. Que reembolsável tem a ver com reembolso e que é mediante a nota fiscal. Excelente. E a pergunta da Capivara? Vamos lá, estou animado para ver. A outra, do quero cinco dias de como faço para solicitar cinco dias você deve formalizar junto a RH, a terceira do curso, sim, cursos da Alura são reembolsados desde que esteja a aprovação prévia. Quantas Capivaras tem no Rio e Pinheiros? Não sei. Pessoal, parabéns para a gente. Funcionou maravilhosamente bem. E agora eu acho que só para dar uma facilitada na vida de vocês, a gente exibiu aqui as citações desse jeito horrível, feio aqui, que não é o ideal que você exiba para um cliente, para um usuário final. Então para isso a gente preparou aqui um código especial para vocês. Para você não ter que digitar do nada, ele vai estar aqui também na descrição dessa aula, para você só dar um control C e control V, ou também lá no Colab, nesse notebook do Colab que você pode seguir com a gente. Mas ele é um código gigantesco que eu vou colar ele aqui antes da nossa pergunta política RAG. Então dando o control V agora para vocês, você vai ver que é só um formatador, tem um monte de reggae, de expressões regulares. Isso aqui é só para deixar bonito. É só para formatar texto, não tem nada do agente, de eda, só para formatar texto. Não é lógica. Exato. Então vamos rodar essa célula aqui antes da política RAG. E aí lá dentro do perguntar política RAG da função, em vez de retornar só o docs relacionados, eu vou formatar, vou deixar bonitinho usando essa função nova que a gente acabou de adicionar, que a gente vai passar para ele os docs relacionados e a pergunta. Correto? Perfeito. Então é uma alteração que a gente vai fazer nessa função. Roda de novo para guardar a memória. E aí lá no for também, em vez de só dar esse print nas citações, a gente vai ter que fazer um forzinho aqui um pouco mais bonitinho, eu acho, para exibir isso. Então vou apagar o resposta citações do print. E o que eu vou fazer? Olha, ele está até sugerindo para a gente aqui. Vamos completar e vamos ver. Então vamos lá. Por ser em resposta, então ele está olhando todas que estão dentro de resposta citação, perfeito. Porque pode ter mais de uma citação para uma pergunta, para fazer o concreto. Porque a gente colocou lá o chunk de tamanho 4. Então pode ter até 4. Pode ter menos, mas pode ter até o máximo 4. Legal. Então para cada uma... A gente bota no ser e aí vai lá, documento. É o ser no nome do documento, aí a página onde ele encontrou isso, ele sugeriu isso, que pode ser. E o trecho. E o trecho específico onde ele encontrou isso. Ficou legal. Eu gostei, vamos testar. Vamos testar só assim, do jeito que ele sugeriu para a gente. Isso é bem melhor. Maravilha, muito mais fácil. Isso é muito legal, porque quando a gente cria o agente, uma das coisas mais complexas do agente é fazer o monitoramento dele. Saber se ele está respondendo certo ou não. E fazer esse tipo de organização ajuda muito. Por exemplo, por que? Será que ele pegou o chunk errado? Será que o documento não tinha informação correta? Então isso aqui ajuda muito. E se você quiser listar também, tipo, ah, putz, isso aqui é o documento tal. Aqui ele está buscando o nome do documento. No documento política de reembolso, viagem e despesas, na página 1, o trecho. L-savings, que deve ser o reembolsavings. Então, como a gente falou antes. E aí, transporte, se a gente for mais para a direita, aqui a gente vai encontrar algum momento. Então, internet para home office, reembolsável via subsistível mensal. Esse foi o primeiro chunk. E ele usou outro chunk, que é aqui, ó, de conectividade do outro documento. De política de home office, a subsistível mensal de internet domiciliária, até mediante não-nofiscal dominal. Então, aquilo que eu falei, que eu achei que ele tinha pego daqui da primeira linha, na verdade não. Ele pegou esse cara aqui, né, do internet para home office. E também o cara da outra, do outro do PDF, que é esse daqui do conectividade. Perfeito. Que é mais difícil até. Ele pegou de documentos diferentes, mas conseguiu conectar. Pedaços de documentos diferentes, mas que conectavam porque tinha essa similaridade semântica do embedding. E aí ele juntou tudo para gerar resposta. Perfeito. Muito legal. E aí a mesma coisa aqui para o 5 dias de trabalho remoto, né, de política de home office, política de reembols, que não tem muito a ver, né, mas ele conseguiu gerar aqui, ó, você tem que formalizar junto ao RH. Aqui também dos treinamentos da Alura. Pegou os dois de documentos diferentes. E o da Capivara, como não tem documento relacionado, não sei. Não tem citação. Simplesmente. Maravilha, pessoal. Terminamos. Essa foi a aula 2, que eu falei que ia ser uma aula um pouquinho mais densa, né, porque é o conceito do Rague que a gente vai utilizar lá no nosso agente, na aula 3. Mas agora praticamente está tudo pronto. Só falta a gente criar a estrutura do agente, que a gente vai conectar ali as arestas, os nós do nosso agente. Isso tudo a gente vai fazer na próxima aula. Novamente, os recadinhos finais aqui. Se você ficou com qualquer dúvida, você pode perguntar para o Gemini aqui dentro do próprio Google Collab, ou então na página do Gemini. Também pode fazer isso sem nenhum problema. Ou também lá na comunidade do Discord da Alura, que você... Aqui da imersão, na verdade, né, da imersão agentes de IA, onde você vai compartilhar as coisas que você fez, as suas dúvidas, e o pessoal vai estar lá. Tanto o pessoal da comunidade, né, para tirar as dúvidas, quanto o nosso pessoal da Alura para ajudar vocês lá, aqui com o conteúdo dessa imersão. E também a gente quer ver o que você está fazendo. Então vai fazendo as coisas e vai compartilhando nas redes sociais, né, no seu LinkedIn, no seu X, no seu Instagram, as coisas que você está desenvolvendo. E a gente sempre incentiva vocês a irem além. Então a gente está fazendo tudo em código. O que você acha de botar uma coisa mais visual aqui, né? Tem várias bibliotecas open source, como o Streamlit, por exemplo, o Gradle, que você pode fazer de uma forma, uma exibição mais bonita de tudo que está aqui. E se você não sabe como usar, pergunta para o Geminai, pergunta na comunidade, né? O pessoal vai te ajudar a criar um projetinho cada vez mais bonito com o que a gente tem aqui. Tudo feito? Tudo certo, pessoal? Excelente. Então amanhã a gente se vê na última aula, a aula 3 dessa imersão de agentes de IA da Alura com o Google Geminai. Tchau, tchau, pessoal. Até mais. Tchau, pessoal.